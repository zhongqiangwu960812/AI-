# 数据集描述

后面拟采用一个统一的稍微正规的数据集，是新闻推荐的一个数据集，来自于某个推荐比赛，因为这个数据集是来自工业上的真实数据，所以使用起来比之前用的movielens数据集可尝试的东西多一些，并且原数据有8个多G，总共3个文件: 用户画像，文章画像， 点击日志，用户数量100多万，6000多万次点击， 文章规模是几百，数据量也比较丰富， 用户的历史行为记录也超级丰富。 **这样可以使得所有的精排模型或者召回模型都可以在这上面做实验，这样也能对比各个模型之间的效果**

全量数据集链接(链接失效可以在博客留言呀)

* 链接：https://pan.baidu.com/s/10hvG4aBNwCitGINs6XVjWw 
* 提取码：wh2s
## 数据采样(点击日志数据集初步处理与采样.ipynb)

这个数据集由于太大，我这边电脑无法直接跑，我依然是采用了一些策略进行采样，选择出了一份比较规整的数据集，20000用户的100多万的历史点击， 做实验用也足够了，小本子也能跑起来。 采样逻辑如下:

1. 分块读取数据， 无法一下子读入内存
2. 对于每块数据，基于一些筛选规则进行记录的删除，比如只用了后7天的数据， 删除了一些文章不在物料池的数据， 删除不合法的点击记录(曝光时间大于文章上传时间)， 删除没有历史点击的用户，删除观看时间低于3s的视频， 删除历史点击序列太短和太长的用户记录
3. 删除完之后重新保存一份新数据集，大约3个G，然后再从这里面随机采样了20000用户进行了后面实验

## 数据预处理(EDA与数据预处理.ipynb)

这个也是写成了一个笔记本， 主要是看了下采样后的数据，序列长度分布等，由于上面做了一些规整化，这里有毛病的数据不是太多，并没有太多处理， 但是用户数据里面的年龄，性别源数据是给出了多种可能， 每个可能有概率值，我这里选出了概率最大的那个，然后简单填充了缺失。

最后把能用到的用户画像和文章画像统一拼接到了点击日志数据，又保存了一份。 作为YouTubeDNN模型的使用数据， 其他模型我也打算使用这份数据了。

为了方便同学快速做实验， 我也把我采样之后的数据集上传了上来， 不想重新下载全量数据，不想重新处理的可以用这份数据。因为8个G的数据集处理起来，分块读入的时候也是超级慢的。这个是data_process， 里面是处理好的数据(由于GitHub有大小限制，这里还是传百度云吧)

我后面模型试验，代码读取数据的时候，也是直接从这里面读取的。这样只需要：

```
data_path = 'data_process'
data = pd.read_csv(os.path.join(data_path, 'train_data.csv'), index_col=0, parse_dates=['expo_time'])
```

## 特别声明

数据只是单纯的供学习使用，不能用作任何商业用途。

